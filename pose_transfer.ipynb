{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-27T08:58:58.139785798Z",
     "start_time": "2023-10-27T08:58:55.925443747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/videos/remastered_omco_pickingup_1 (video-converter.com).mp4\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "Setup_environment()\n",
    "\n",
    "from hpe_wrapper import Wrapper_2Dpose, Wrapper_3Dpose\n",
    "\n",
    "model_2D ='./detectron/configs/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml'\n",
    "weights_2D = './detectron/checkpoint/model_final_997cc7.pkl'\n",
    "model_3D = './VideoPose3D/checkpoint/pretrained_h36m_detectron_coco.bin'\n",
    "\n",
    "pose2d = Wrapper_2Dpose(model=model_2D, weights= weights_2D , ROI_thr=0.75)\n",
    "pose_3d = Wrapper_3Dpose(model_3D)\n",
    "\n",
    "config  = load_config()\n",
    "print(config['video_object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0 processed in 1.043s\n",
      "Frame 1 processed in 0.078s\n",
      "Frame 2 processed in 0.091s\n",
      "Frame 3 processed in 0.094s\n",
      "Frame 4 processed in 0.102s\n",
      "Frame 5 processed in 0.106s\n",
      "Frame 6 processed in 0.090s\n",
      "Frame 7 processed in 0.083s\n",
      "Frame 8 processed in 0.083s\n",
      "Frame 9 processed in 0.092s\n",
      "Frame 10 processed in 0.092s\n",
      "Frame 11 processed in 0.085s\n",
      "Frame 12 processed in 0.081s\n",
      "Frame 13 processed in 0.078s\n",
      "Frame 14 processed in 0.085s\n",
      "Frame 15 processed in 0.086s\n",
      "Frame 16 processed in 0.090s\n",
      "Frame 17 processed in 0.093s\n",
      "Frame 18 processed in 0.092s\n",
      "Frame 19 processed in 0.077s\n",
      "Frame 20 processed in 0.091s\n",
      "Frame 21 processed in 0.092s\n",
      "Frame 22 processed in 0.078s\n",
      "Frame 23 processed in 0.085s\n",
      "Frame 24 processed in 0.090s\n",
      "Frame 25 processed in 0.077s\n",
      "Frame 26 processed in 0.083s\n",
      "Frame 27 processed in 0.082s\n",
      "Frame 28 processed in 0.079s\n",
      "Frame 29 processed in 0.082s\n",
      "Frame 30 processed in 0.086s\n",
      "Frame 31 processed in 0.082s\n",
      "Frame 32 processed in 0.086s\n",
      "Frame 33 processed in 0.093s\n",
      "Frame 34 processed in 0.082s\n",
      "Frame 35 processed in 0.083s\n",
      "Frame 36 processed in 0.090s\n",
      "Frame 37 processed in 0.090s\n",
      "Frame 38 processed in 0.088s\n",
      "Frame 39 processed in 0.092s\n",
      "Frame 40 processed in 0.082s\n",
      "Frame 41 processed in 0.082s\n",
      "Frame 42 processed in 0.089s\n",
      "Frame 43 processed in 0.081s\n",
      "Frame 44 processed in 0.080s\n",
      "Frame 45 processed in 0.082s\n",
      "Frame 46 processed in 0.081s\n",
      "Frame 47 processed in 0.081s\n",
      "Frame 48 processed in 0.086s\n",
      "Frame 49 processed in 0.084s\n",
      "Frame 50 processed in 0.084s\n",
      "Frame 51 processed in 0.088s\n",
      "Frame 52 processed in 0.079s\n",
      "Frame 53 processed in 0.083s\n",
      "Frame 54 processed in 0.089s\n",
      "Frame 55 processed in 0.081s\n",
      "Frame 56 processed in 0.085s\n",
      "Frame 57 processed in 0.080s\n",
      "Frame 58 processed in 0.081s\n",
      "Frame 59 processed in 0.082s\n",
      "Frame 60 processed in 0.092s\n",
      "Frame 61 processed in 0.092s\n",
      "Frame 62 processed in 0.093s\n",
      "Frame 63 processed in 0.091s\n",
      "Frame 64 processed in 0.081s\n",
      "Frame 65 processed in 0.086s\n",
      "Frame 66 processed in 0.084s\n",
      "Frame 67 processed in 0.082s\n",
      "Frame 68 processed in 0.092s\n",
      "Frame 69 processed in 0.095s\n",
      "Frame 70 processed in 0.097s\n",
      "Frame 71 processed in 0.110s\n",
      "Frame 72 processed in 0.096s\n",
      "Frame 73 processed in 0.081s\n",
      "Frame 74 processed in 0.096s\n",
      "Frame 75 processed in 0.084s\n",
      "Frame 76 processed in 0.089s\n",
      "Frame 77 processed in 0.093s\n",
      "Frame 78 processed in 0.086s\n",
      "Frame 79 processed in 0.084s\n",
      "Frame 80 processed in 0.081s\n",
      "Frame 81 processed in 0.080s\n",
      "Frame 82 processed in 0.081s\n",
      "Frame 83 processed in 0.088s\n",
      "Frame 84 processed in 0.084s\n",
      "Frame 85 processed in 0.083s\n",
      "Frame 86 processed in 0.087s\n",
      "Frame 87 processed in 0.092s\n",
      "Frame 88 processed in 0.088s\n",
      "Frame 89 processed in 0.089s\n",
      "Frame 90 processed in 0.088s\n",
      "Frame 91 processed in 0.089s\n",
      "Frame 92 processed in 0.088s\n",
      "Frame 93 processed in 0.091s\n",
      "Frame 94 processed in 0.088s\n",
      "Frame 95 processed in 0.088s\n",
      "Frame 96 processed in 0.084s\n",
      "Frame 97 processed in 0.081s\n",
      "Frame 98 processed in 0.079s\n",
      "Frame 99 processed in 0.083s\n",
      "Frame 100 processed in 0.078s\n",
      "Frame 101 processed in 0.084s\n",
      "Frame 102 processed in 0.089s\n",
      "Frame 103 processed in 0.081s\n",
      "Frame 104 processed in 0.093s\n",
      "Frame 105 processed in 0.088s\n",
      "Frame 106 processed in 0.080s\n",
      "Frame 107 processed in 0.089s\n",
      "Frame 108 processed in 0.093s\n",
      "Frame 109 processed in 0.088s\n",
      "Frame 110 processed in 0.092s\n",
      "Frame 111 processed in 0.088s\n",
      "Frame 112 processed in 0.082s\n",
      "Frame 113 processed in 0.087s\n",
      "Frame 114 processed in 0.091s\n",
      "Frame 115 processed in 0.082s\n",
      "Frame 116 processed in 0.080s\n",
      "Frame 117 processed in 0.084s\n",
      "Frame 118 processed in 0.137s\n",
      "Frame 119 processed in 0.115s\n",
      "Frame 120 processed in 0.108s\n",
      "Frame 121 processed in 0.108s\n",
      "Frame 122 processed in 0.084s\n",
      "Frame 123 processed in 0.095s\n",
      "Frame 124 processed in 0.109s\n",
      "Frame 125 processed in 0.094s\n",
      "Frame 126 processed in 0.091s\n",
      "Frame 127 processed in 0.082s\n",
      "Frame 128 processed in 0.088s\n",
      "Frame 129 processed in 0.087s\n",
      "Frame 130 processed in 0.090s\n",
      "Frame 131 processed in 0.087s\n",
      "Frame 132 processed in 0.086s\n",
      "Frame 133 processed in 0.084s\n",
      "Frame 134 processed in 0.091s\n",
      "Frame 135 processed in 0.093s\n",
      "Frame 136 processed in 0.086s\n",
      "Frame 137 processed in 0.088s\n",
      "Frame 138 processed in 0.088s\n",
      "Frame 139 processed in 0.080s\n",
      "Frame 140 processed in 0.096s\n",
      "Frame 141 processed in 0.091s\n",
      "Frame 142 processed in 0.089s\n",
      "Frame 143 processed in 0.100s\n",
      "Frame 144 processed in 0.095s\n",
      "Frame 145 processed in 0.083s\n",
      "Frame 146 processed in 0.091s\n",
      "Frame 147 processed in 0.086s\n",
      "Frame 148 processed in 0.084s\n",
      "Frame 149 processed in 0.093s\n",
      "Frame 150 processed in 0.094s\n",
      "Frame 151 processed in 0.083s\n",
      "Frame 152 processed in 0.093s\n",
      "Frame 153 processed in 0.086s\n",
      "Frame 154 processed in 0.082s\n",
      "Frame 155 processed in 0.090s\n",
      "Frame 156 processed in 0.096s\n",
      "Frame 157 processed in 0.084s\n",
      "Frame 158 processed in 0.092s\n",
      "Frame 159 processed in 0.097s\n",
      "Frame 160 processed in 0.087s\n",
      "Frame 161 processed in 0.086s\n",
      "Frame 162 processed in 0.087s\n",
      "Frame 163 processed in 0.079s\n",
      "Frame 164 processed in 0.087s\n",
      "Frame 165 processed in 0.084s\n",
      "Frame 166 processed in 0.084s\n",
      "Frame 167 processed in 0.083s\n",
      "Finished processing video!\n",
      "Lifting 2D Poses into 3D...\n",
      "Finished 3D pose reconstruction!\n"
     ]
    }
   ],
   "source": [
    "video_object = Video_wrapper(config['video_object'], resize_video_by=0.3) #, start=0, end=10) # downsample_fps_by=2,\n",
    "data_2d, metadata_vid = pose2d.predict_2D_poses(input_video_object=video_object)\n",
    "data_3d = pose_3d.predict_3D_poses(data_2d, metadata_vid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T08:59:51.105715748Z",
     "start_time": "2023-10-27T08:59:33.189096013Z"
    }
   },
   "id": "ac8540920d4e296b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "--- Frame 0: ---\n",
      "Frame: tensor([ 14.3200, 111.9615,  36.7538, 119.7843,  29.0617,  43.0275,   9.8064,\n",
      "         23.7113,  14.8478,   1.2035,  91.7161,  35.1584,  99.9522,  98.7746])\n",
      "Max shoulder: tensor(36.7538)\n",
      "Max abduction: tensor(119.7843)\n",
      "Max elbow: tensor(43.0275)\n",
      "Score Upper Arm: tensor([2.])\n",
      "Score Lower Arm: tensor([1.])\n",
      "Current Score A: tensor(2.)\n",
      "Score Neck: tensor([1.])\n",
      "Score Trunk: tensor([1.])\n",
      "Min knee: tensor(9.8064)\n",
      "Score Legs: tensor([1.])\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'curr_score_B' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 9\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(pose\u001B[38;5;241m.\u001B[39mrequires_grad)\n\u001B[1;32m      7\u001B[0m rula_eval \u001B[38;5;241m=\u001B[39m RULAXXX(pose)\n\u001B[0;32m----> 9\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mrula_eval\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_scores\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(scores\u001B[38;5;241m.\u001B[39mrequires_grad)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#rula_eval.plot_scores(fps=video_object.fps)\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/Thesis/corrective_ergonomics/tensor_ergonomics.py:169\u001B[0m, in \u001B[0;36mRULAXXX.compute_scores\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"score_B.append(curr_score_B)                                      \"\"\"\u001B[39;00m\n\u001B[1;32m    168\u001B[0m curr_score_A \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmin(curr_score_A, torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;241m8.0\u001B[39m, requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[0;32m--> 169\u001B[0m curr_score_B \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmin(\u001B[43mcurr_score_B\u001B[49m, torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;241m7.0\u001B[39m, requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m    171\u001B[0m index1 \u001B[38;5;241m=\u001B[39m (curr_score_A \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mlong()\n\u001B[1;32m    172\u001B[0m index2 \u001B[38;5;241m=\u001B[39m (curr_score_B \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mlong()\n",
      "\u001B[0;31mUnboundLocalError\u001B[0m: local variable 'curr_score_B' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from tensor_ergonomics import RULAXXX\n",
    "original_pose = torch.tensor(data_3d)\n",
    "\n",
    "pose = original_pose.clone().detach().requires_grad_(True)\n",
    "print(pose.requires_grad)\n",
    "\n",
    "rula_eval = RULAXXX(pose)\n",
    "\n",
    "scores = rula_eval.compute_scores()\n",
    "print(scores.requires_grad)\n",
    "\n",
    "#rula_eval.plot_scores(fps=video_object.fps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T09:00:08.156180125Z",
     "start_time": "2023-10-27T09:00:07.592004453Z"
    }
   },
   "id": "68419b7293613b8d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mscores\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "scores[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T09:00:32.995020329Z",
     "start_time": "2023-10-27T09:00:32.948857687Z"
    }
   },
   "id": "ac08ec06edf93d2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pose Optim  Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94c12b909c4a3d0e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3731129540.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[5], line 3\u001B[0;36m\u001B[0m\n\u001B[0;31m    not this one\u001B[0m\n\u001B[0m             ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from xxx_ergonomics import RULAXXX\n",
    "not this one\n",
    "def run_pose_optimization(original_pose, num_steps=300):\n",
    "    \"\"\"\n",
    "    Optimize the pose using the RULA score.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_pose: A tensor of shape (17, 3)\n",
    "    \n",
    "    Returns:\n",
    "    - Optimized pose.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the original_pose is a tensor\n",
    "    if not isinstance(original_pose, torch.Tensor):\n",
    "        original_pose = torch.tensor(original_pose)\n",
    "\n",
    "    pose = original_pose.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Define an optimizer. Here, we use the Adam optimizer.\n",
    "    optimizer = torch.optim.Adam([pose], lr=0.01) #(1,200t, 17, 3)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Convert the tensor to the format expected by RULA (ndarray)\n",
    "        #pose_for_rula = pose.detach().cpu().numpy()\n",
    "\n",
    "        # Compute the RULA score for the current pose            \n",
    "        rula_eval = RULAXXX(pose)\n",
    "        rula_score = rula_eval.compute_scores()\n",
    "        print(rula_score.requires_grad)\n",
    "        ergo_sum = rula_score.sum()\n",
    "        print(ergo_sum.requires_grad)\n",
    "\n",
    "        # Convert the RULA score back to a tensor\n",
    "        #rula_score_tensor = torch.tensor(ergo_sum, dtype=torch.float32)\n",
    "\n",
    "        # We want to minimize the RULA score, so we negate it for optimization\n",
    "        loss = ergo_sum\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the pose\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the RULA score every 50 steps\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: RULA Score: {ergo_sum.item()}\")\n",
    "\n",
    "    return pose\n",
    "\n",
    "\n",
    "\n",
    "optimized_pose = run_pose_optimization(data_3d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T12:07:25.869592959Z",
     "start_time": "2023-10-17T12:07:25.868309745Z"
    }
   },
   "id": "b1485cb3c3ffdef5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ergonomics_torch import RULATorch\n",
    "aaa\n",
    "# Ensure the original_pose is a tensor\n",
    "if not isinstance(data_3d, torch.Tensor):\n",
    "    original_pose = torch.tensor(data_3d)\n",
    "\n",
    "pose = original_pose.clone().detach().requires_grad_(True)\n",
    "\n",
    "rula_eval = RULATorch(original_pose)\n",
    "print(type(original_pose))\n",
    "scores = rula_eval.compute_scores()\n",
    "\n",
    "rula_eval.plot_scores(fps=video_object.fps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-17T12:07:25.868416626Z"
    }
   },
   "id": "3be14035486c0ad0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from ergonomics import RULA\n",
    "\n",
    "class RULALossFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pose):\n",
    "        pose_np = pose.detach().cpu().numpy()\n",
    "        rula_eval = RULA(pose_np)\n",
    "        rula_score = rula_eval.compute_scores()\n",
    "        ctx.save_for_backward(pose)\n",
    "        return torch.tensor(rula_score, dtype=torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a dummy backward function. It assumes that the gradient\n",
    "        # with respect to the pose is not dependent on the RULA score.\n",
    "        # You might need to adjust this based on your actual requirements.\n",
    "        pose, = ctx.saved_tensors\n",
    "        grad_pose = torch.zeros_like(pose)\n",
    "        return grad_pose\n",
    "\n",
    "def run_pose_optimization(original_pose, num_steps=300):\n",
    "    \"\"\"\n",
    "    Optimize the pose using the RULA score.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_pose: A tensor of shape (17, 3)\n",
    "    \n",
    "    Returns:\n",
    "    - Optimized pose.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the original_pose is a tensor\n",
    "    if not isinstance(original_pose, torch.Tensor):\n",
    "        original_pose = torch.tensor(original_pose)\n",
    "\n",
    "    pose = original_pose.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Define an optimizer. Here, we use the Adam optimizer.\n",
    "    optimizer = torch.optim.Adam([pose], lr=0.01)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the RULA score for the current pose using the custom function\n",
    "        rula_score_tensor = RULALossFunction.apply(pose)\n",
    "\n",
    "        # We want to minimize the RULA score, so we negate it for optimization\n",
    "        loss = -rula_score_tensor\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the pose\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the RULA score every 50 steps\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: RULA Score: {rula_score_tensor.item()}\")\n",
    "\n",
    "    return pose\n",
    "\n",
    "original_pose = data_dict['kps'][:1,::][0]\n",
    "original_pose_tensor = torch.tensor(original_pose, dtype=torch.float32, requires_grad=True)\n",
    "print(original_pose_tensor.shape) #torch.Size([17, 3])\n",
    "optimized_pose = run_pose_optimization(original_pose_tensor)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-17T12:07:25.868461863Z"
    }
   },
   "id": "ddf0feb9b6c4bf91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from ergonomics import RULA\n",
    "\n",
    "def run_pose_optimization(original_pose, num_steps=300):\n",
    "    \"\"\"\n",
    "    Optimize the pose using the RULA score.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_pose: A tensor of shape (17, 3)\n",
    "    \n",
    "    Returns:\n",
    "    - Optimized pose.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the pose to be optimized\n",
    "    pose = original_pose.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Define an optimizer. Here, we use the Adam optimizer.\n",
    "    optimizer = torch.optim.Adam([pose], lr=0.01)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the RULA score for the current pose            \n",
    "        rula_eval = RULA(original_pose)\n",
    "        print(rula_eval)\n",
    "        rula_score = rula_eval.compute_scores()\n",
    "        \n",
    "        # We want to minimize the RULA score, so we negate it for optimization\n",
    "        loss = -rula_score\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the pose\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the RULA score every 50 steps\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: RULA Score: {rula_score.item()}\")\n",
    "\n",
    "    return pose\n",
    "\n",
    "\n",
    "# Optimize the pose\n",
    "original_pose = data_dict['kps'][:1,::][0]\n",
    "original_pose_tensor = torch.tensor(original_pose, dtype=torch.float32, requires_grad=True)\n",
    "print(original_pose_tensor.shape) #torch.Size([17, 3])\n",
    "optimized_pose = run_pose_optimization(original_pose_tensor)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-17T12:07:25.868527934Z"
    }
   },
   "id": "f3db0c248d7e3121"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-17T12:07:25.868572381Z"
    }
   },
   "id": "b51934cb387fbb6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
