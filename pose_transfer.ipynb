{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:03:02.459518973Z",
     "start_time": "2023-11-02T16:02:46.279852770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/videos/remastered_omco_pickingup_1 (video-converter.com).mp4\n",
      "Frame 0 processed in 1.119s\n",
      "Frame 1 processed in 0.101s\n",
      "Frame 2 processed in 0.129s\n",
      "Frame 3 processed in 0.070s\n",
      "Frame 4 processed in 0.069s\n",
      "Frame 5 processed in 0.079s\n",
      "Frame 6 processed in 0.069s\n",
      "Frame 7 processed in 0.066s\n",
      "Frame 8 processed in 0.068s\n",
      "Frame 9 processed in 0.069s\n",
      "Frame 10 processed in 0.061s\n",
      "Frame 11 processed in 0.064s\n",
      "Frame 12 processed in 0.059s\n",
      "Frame 13 processed in 0.059s\n",
      "Frame 14 processed in 0.059s\n",
      "Frame 15 processed in 0.061s\n",
      "Frame 16 processed in 0.063s\n",
      "Frame 17 processed in 0.060s\n",
      "Frame 18 processed in 0.062s\n",
      "Frame 19 processed in 0.059s\n",
      "Frame 20 processed in 0.060s\n",
      "Frame 21 processed in 0.061s\n",
      "Frame 22 processed in 0.062s\n",
      "Frame 23 processed in 0.062s\n",
      "Frame 24 processed in 0.061s\n",
      "Frame 25 processed in 0.058s\n",
      "Frame 26 processed in 0.058s\n",
      "Frame 27 processed in 0.058s\n",
      "Frame 28 processed in 0.059s\n",
      "Frame 29 processed in 0.058s\n",
      "Frame 30 processed in 0.058s\n",
      "Frame 31 processed in 0.058s\n",
      "Frame 32 processed in 0.062s\n",
      "Frame 33 processed in 0.057s\n",
      "Frame 34 processed in 0.059s\n",
      "Frame 35 processed in 0.057s\n",
      "Frame 36 processed in 0.059s\n",
      "Frame 37 processed in 0.058s\n",
      "Frame 38 processed in 0.058s\n",
      "Frame 39 processed in 0.061s\n",
      "Frame 40 processed in 0.058s\n",
      "Frame 41 processed in 0.057s\n",
      "Frame 42 processed in 0.058s\n",
      "Frame 43 processed in 0.057s\n",
      "Frame 44 processed in 0.058s\n",
      "Frame 45 processed in 0.060s\n",
      "Frame 46 processed in 0.058s\n",
      "Frame 47 processed in 0.057s\n",
      "Frame 48 processed in 0.058s\n",
      "Frame 49 processed in 0.059s\n",
      "Frame 50 processed in 0.057s\n",
      "Frame 51 processed in 0.058s\n",
      "Frame 52 processed in 0.059s\n",
      "Frame 53 processed in 0.058s\n",
      "Frame 54 processed in 0.058s\n",
      "Frame 55 processed in 0.057s\n",
      "Frame 56 processed in 0.058s\n",
      "Frame 57 processed in 0.057s\n",
      "Frame 58 processed in 0.059s\n",
      "Frame 59 processed in 0.058s\n",
      "Frame 60 processed in 0.059s\n",
      "Frame 61 processed in 0.059s\n",
      "Frame 62 processed in 0.058s\n",
      "Frame 63 processed in 0.059s\n",
      "Frame 64 processed in 0.060s\n",
      "Frame 65 processed in 0.059s\n",
      "Frame 66 processed in 0.058s\n",
      "Frame 67 processed in 0.058s\n",
      "Frame 68 processed in 0.061s\n",
      "Frame 69 processed in 0.060s\n",
      "Frame 70 processed in 0.058s\n",
      "Frame 71 processed in 0.058s\n",
      "Frame 72 processed in 0.060s\n",
      "Frame 73 processed in 0.061s\n",
      "Frame 74 processed in 0.058s\n",
      "Frame 75 processed in 0.061s\n",
      "Frame 76 processed in 0.059s\n",
      "Frame 77 processed in 0.058s\n",
      "Frame 78 processed in 0.058s\n",
      "Frame 79 processed in 0.060s\n",
      "Frame 80 processed in 0.059s\n",
      "Frame 81 processed in 0.060s\n",
      "Frame 82 processed in 0.058s\n",
      "Frame 83 processed in 0.058s\n",
      "Frame 84 processed in 0.059s\n",
      "Frame 85 processed in 0.059s\n",
      "Frame 86 processed in 0.059s\n",
      "Frame 87 processed in 0.058s\n",
      "Frame 88 processed in 0.061s\n",
      "Frame 89 processed in 0.058s\n",
      "Frame 90 processed in 0.058s\n",
      "Frame 91 processed in 0.059s\n",
      "Frame 92 processed in 0.060s\n",
      "Frame 93 processed in 0.062s\n",
      "Frame 94 processed in 0.058s\n",
      "Frame 95 processed in 0.059s\n",
      "Frame 96 processed in 0.061s\n",
      "Frame 97 processed in 0.059s\n",
      "Frame 98 processed in 0.058s\n",
      "Frame 99 processed in 0.060s\n",
      "Frame 100 processed in 0.061s\n",
      "Frame 101 processed in 0.060s\n",
      "Frame 102 processed in 0.058s\n",
      "Frame 103 processed in 0.058s\n",
      "Frame 104 processed in 0.058s\n",
      "Frame 105 processed in 0.057s\n",
      "Frame 106 processed in 0.057s\n",
      "Frame 107 processed in 0.059s\n",
      "Frame 108 processed in 0.060s\n",
      "Frame 109 processed in 0.060s\n",
      "Frame 110 processed in 0.058s\n",
      "Frame 111 processed in 0.058s\n",
      "Frame 112 processed in 0.059s\n",
      "Frame 113 processed in 0.059s\n",
      "Frame 114 processed in 0.059s\n",
      "Frame 115 processed in 0.057s\n",
      "Frame 116 processed in 0.058s\n",
      "Frame 117 processed in 0.058s\n",
      "Frame 118 processed in 0.058s\n",
      "Frame 119 processed in 0.060s\n",
      "Frame 120 processed in 0.060s\n",
      "Frame 121 processed in 0.058s\n",
      "Frame 122 processed in 0.058s\n",
      "Frame 123 processed in 0.058s\n",
      "Frame 124 processed in 0.058s\n",
      "Frame 125 processed in 0.062s\n",
      "Frame 126 processed in 0.059s\n",
      "Frame 127 processed in 0.057s\n",
      "Frame 128 processed in 0.058s\n",
      "Frame 129 processed in 0.058s\n",
      "Frame 130 processed in 0.059s\n",
      "Frame 131 processed in 0.058s\n",
      "Frame 132 processed in 0.060s\n",
      "Frame 133 processed in 0.058s\n",
      "Frame 134 processed in 0.057s\n",
      "Frame 135 processed in 0.058s\n",
      "Frame 136 processed in 0.061s\n",
      "Frame 137 processed in 0.058s\n",
      "Frame 138 processed in 0.058s\n",
      "Frame 139 processed in 0.058s\n",
      "Frame 140 processed in 0.059s\n",
      "Frame 141 processed in 0.058s\n",
      "Frame 142 processed in 0.058s\n",
      "Frame 143 processed in 0.058s\n",
      "Frame 144 processed in 0.059s\n",
      "Frame 145 processed in 0.058s\n",
      "Frame 146 processed in 0.059s\n",
      "Frame 147 processed in 0.060s\n",
      "Frame 148 processed in 0.059s\n",
      "Frame 149 processed in 0.058s\n",
      "Frame 150 processed in 0.059s\n",
      "Frame 151 processed in 0.059s\n",
      "Frame 152 processed in 0.059s\n",
      "Frame 153 processed in 0.075s\n",
      "Frame 154 processed in 0.058s\n",
      "Frame 155 processed in 0.058s\n",
      "Frame 156 processed in 0.061s\n",
      "Frame 157 processed in 0.059s\n",
      "Frame 158 processed in 0.058s\n",
      "Frame 159 processed in 0.058s\n",
      "Frame 160 processed in 0.059s\n",
      "Frame 161 processed in 0.062s\n",
      "Frame 162 processed in 0.057s\n",
      "Frame 163 processed in 0.057s\n",
      "Frame 164 processed in 0.059s\n",
      "Frame 165 processed in 0.058s\n",
      "Frame 166 processed in 0.058s\n",
      "Frame 167 processed in 0.058s\n",
      "Finished processing video!\n",
      "Lifting 2D Poses into 3D...\n",
      "Finished 3D pose reconstruction!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "Setup_environment()\n",
    "\n",
    "from hpe_wrapper import Wrapper_2Dpose, Wrapper_3Dpose\n",
    "\n",
    "model_2D ='./detectron/configs/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml'\n",
    "weights_2D = './detectron/checkpoint/model_final_997cc7.pkl'\n",
    "model_3D = './VideoPose3D/checkpoint/pretrained_h36m_detectron_coco.bin'\n",
    "\n",
    "pose2d = Wrapper_2Dpose(model=model_2D, weights= weights_2D , ROI_thr=0.75)\n",
    "pose_3d = Wrapper_3Dpose(model_3D)\n",
    "\n",
    "config  = load_config()\n",
    "print(config['video_object'])\n",
    "\n",
    "video_object = Video_wrapper(config['video_object'], resize_video_by=0.3) #, start=0, end=10) # downsample_fps_by=2,\n",
    "data_2d, metadata_vid = pose2d.predict_2D_poses(input_video_object=video_object)\n",
    "data_3d = pose_3d.predict_3D_poses(data_2d, metadata_vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pose Optim  Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94c12b909c4a3d0e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 38\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStep \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, RULA Score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrula_score\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_pose\n\u001B[0;32m---> 38\u001B[0m optimized_pose \u001B[38;5;241m=\u001B[39m \u001B[43mrun_pose_optimization\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_3d\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m p \u001B[38;5;241m=\u001B[39m optimized_pose\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOriginal pose: \u001B[39m\u001B[38;5;124m'\u001B[39m, data_3d[\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,])\n",
      "Cell \u001B[0;32mIn[2], line 26\u001B[0m, in \u001B[0;36mrun_pose_optimization\u001B[0;34m(original_pose, num_steps)\u001B[0m\n\u001B[1;32m     23\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     25\u001B[0m rula_eval \u001B[38;5;241m=\u001B[39m RULAXXX(original_pose)\n\u001B[0;32m---> 26\u001B[0m rula_score \u001B[38;5;241m=\u001B[39m \u001B[43mrula_eval\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_scores\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(rula_score\u001B[38;5;241m.\u001B[39mgrad_fn)\n\u001B[1;32m     28\u001B[0m ergo_sum \u001B[38;5;241m=\u001B[39m rula_score\u001B[38;5;241m.\u001B[39msum()\n",
      "File \u001B[0;32m~/Projects/Thesis/corrective_ergonomics/tensor_ergonomics.py:72\u001B[0m, in \u001B[0;36mRULAXXX.compute_scores\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     69\u001B[0m score_val \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mwhere((max_shoulder \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m45\u001B[39m) \u001B[38;5;241m&\u001B[39m (max_shoulder \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m90\u001B[39m), torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;241m3.0\u001B[39m, requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), score_val)\n\u001B[1;32m     70\u001B[0m score_val \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mwhere(max_shoulder \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m90\u001B[39m, torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;241m4.0\u001B[39m, requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), score_val)\n\u001B[0;32m---> 72\u001B[0m score_upper_arm \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscore_upper_arm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscore_val\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# A. Check Abduction\u001B[39;00m\n\u001B[1;32m     75\u001B[0m max_abduction \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(frame[\u001B[38;5;241m1\u001B[39m], frame[\u001B[38;5;241m3\u001B[39m])\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tensor_ergonomics import RULAXXX\n",
    "\n",
    "def run_pose_optimization(original_pose, num_steps=3):\n",
    "    \"\"\"\n",
    "    Optimize the pose using the RULA score.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_pose: A tensor of shape (17, 3)\n",
    "    \n",
    "    Returns:\n",
    "    - Optimized pose.\n",
    "    \"\"\"\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    if not isinstance(original_pose, torch.Tensor):\n",
    "        original_pose = torch.tensor(original_pose)\n",
    "\n",
    "    original_pose = original_pose.clone().detach().requires_grad_(True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([original_pose], lr=0.1) #(1,200t, 17, 3)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rula_eval = RULAXXX(original_pose)\n",
    "        rula_score = rula_eval.compute_scores()\n",
    "        print(rula_score.grad_fn)\n",
    "        ergo_sum = rula_score.sum()\n",
    "        loss = ergo_sum\n",
    "\n",
    "        loss.backward()\n",
    "        print(original_pose.grad)\n",
    "        optimizer.step()\n",
    "        print(f\"Step {step}: Loss: {loss.item()}, RULA Score: {rula_score.sum().item()}\")\n",
    "        \n",
    "    return original_pose\n",
    "\n",
    "optimized_pose = run_pose_optimization(data_3d)\n",
    "\n",
    "p = optimized_pose.detach().numpy()\n",
    "print('Original pose: ', data_3d[1,1,])\n",
    "print('Optim pose: ', p[1,1,:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:03:09.459510361Z",
     "start_time": "2023-11-02T16:03:02.459305867Z"
    }
   },
   "id": "b1485cb3c3ffdef5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7e9bdea3145db11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import webbrowser\n",
    "import time\n",
    "\n",
    "def visualize_frame(frame):\n",
    "    # Extract x, y, z coordinates of keypoints\n",
    "    x = frame[:, 0]\n",
    "    y = frame[:, 2]\n",
    "    z = - frame[:, 1]\n",
    "\n",
    "    # Create a scatter plot for the keypoints\n",
    "    scatter = go.Scatter3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color='red')\n",
    "    )\n",
    "\n",
    "    # Define pairs of keypoints to connect with green lines\n",
    "    pairs = [(0,1), (0,4), (0,7), (7,8), (8,9), (9,10), (4,5), (1,2), (5,6), (2,3),\n",
    "             (8,11), (8,14), (11,12), (14,15), (12,13), (15,16)]\n",
    "\n",
    "    lines = []\n",
    "    for pair in pairs:\n",
    "        lines.append(\n",
    "            go.Scatter3d(\n",
    "                x=[x[pair[0]], x[pair[1]]],\n",
    "                y=[y[pair[0]], y[pair[1]]],\n",
    "                z=[z[pair[0]], z[pair[1]]],\n",
    "                mode='lines',\n",
    "                line=dict(color='green', width=5)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Create the layout and add all traces\n",
    "    layout = go.Layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(nticks=10, range=[min(x), max(x)]),\n",
    "            yaxis=dict(nticks=10, range=[min(y), max(y)]),\n",
    "            zaxis=dict(nticks=10, range=[min(z), max(z)])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[scatter] + lines, layout=layout)\n",
    "\n",
    "    # Save the plot as an HTML file\n",
    "    filename = \"plot.html\"\n",
    "    fig.write_html(filename)\n",
    "\n",
    "    # Open the saved HTML file in the default web browser\n",
    "    webbrowser.open(filename)\n",
    "    time.sleep(3)\n",
    "\n",
    "# Example usage\n",
    "# Assuming data_3d is your 3D array of shape (199, 17, 3)\n",
    "for frame in p[9:10,:,:]:\n",
    "    visualize_frame(frame)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8af6ff8cde6d204d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f10985edcccc1ad8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "72ce1d6498fb1b4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ddf0feb9b6c4bf91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensor_ergonomics import RULAXXX\n",
    "\n",
    "def run_pose_optimization(original_pose, num_steps=300):\n",
    "    \"\"\"\n",
    "    Optimize the pose using the RULA score.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_pose: A tensor of shape (17, 3)\n",
    "    \n",
    "    Returns:\n",
    "    - Optimized pose.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the pose to be optimized\n",
    "    pose = original_pose.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Define an optimizer. Here, we use the Adam optimizer.\n",
    "    optimizer = torch.optim.Adam([pose], lr=0.01)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the RULA score for the current pose            \n",
    "        rula_eval = RULAXXX(original_pose)\n",
    "        print(rula_eval)\n",
    "        rula_score = rula_eval.compute_scores()\n",
    "        \n",
    "        # We want to minimize the RULA score, so we negate it for optimization\n",
    "        loss = -rula_score\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the pose\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the RULA score every 50 steps\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: RULA Score: {rula_score.item()}\")\n",
    "\n",
    "    return pose\n",
    "\n",
    "\n",
    "# Optimize the pose\n",
    "original_pose = data_3d\n",
    "original_pose_tensor = torch.tensor(original_pose, dtype=torch.float32, requires_grad=True)\n",
    "print(original_pose_tensor.shape) #torch.Size([17, 3])\n",
    "optimized_pose = run_pose_optimization(original_pose_tensor)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3db0c248d7e3121"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
